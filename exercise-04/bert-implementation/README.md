# BERT Text Classification

## Project Overview
This project implements a BERT-based text classification pipeline using `transformers`, `torch`, and `sklearn`. The code is modular and organized into separate files for data loading, tokenization, dataset handling, training, and evaluation.

## Project Structure
```
ðŸ“‚ bert_project
 â”œâ”€â”€ ðŸ“„ load_data.py                    # Loads data from CSV with dynamic file and column names
 â”œâ”€â”€ ðŸ“„ tokenizer.py                    # Handles tokenization
 â”œâ”€â”€ ðŸ“„ bert_dataset.py                 # Defines the dataset class
 â”œâ”€â”€ ðŸ“„ train.py                        # Training functions
 â”œâ”€â”€ ðŸ“„ evaluate.py                     # Evaluation functions
 â”œâ”€â”€ ðŸ“„ bert_exercise_dmoz_health.ipynb # Jupyter Notebook to run the dmoz health dataset
 â”œâ”€â”€ ðŸ“„ README.md                       # Documentation
```

## Installation
1. Clone the repository:
   ```sh
   git clone https://github.com/MichaelRSilva/unb-natural-language-processing.git
   cd exercise-04
   ```
2. Install dependencies:
   ```sh
   pip install torch transformers scikit-learn pandas matplotlib
   ```

## Usage
### Load Dataset
Modify `bert_exercise_dmoz_health.ipynb` with your dataset file name and column names:
```python
# Define parameters
data_file = "Dmoz-Health.csv"
text_column = "text"
label_column = "class"
```

### Run the Notebook
Open `bert_training.ipynb` and execute all cells.
```sh
jupyter notebook
```

## Results
After running the training, the evaluation metrics (Accuracy, F1-score, and Confusion Matrix) will be displayed.

## Features
âœ… **Supports Custom Datasets** (Specify CSV file & column names)  
âœ… **Modular Codebase** (Separate functions for training, tokenization, etc.)  
âœ… **BERT Model Integration** (Pretrained `bert-base-uncased`)  
âœ… **Train/Test Split (70/10/20)**